{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d4aac1da742c90",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Week 36\n",
    "\n",
    "## Exercise 1\n",
    "### Ex1a)\n",
    "Last week we showed that \n",
    "\\begin{gather}\n",
    "    \\frac{\\partial (\\mathbf{x} - \\mathbf{A}\\mathbf{s})^T(\\mathbf{x} - \\mathbf{A}\\mathbf{s})}{\\partial \\mathbf{s}} = -2(\\mathbf{x} - \\mathbf{A}\\mathbf{s})^T\\mathbf{A}\n",
    "\\end{gather}\n",
    "which then showed that the differentiating the OLS cost function gives\n",
    "\\begin{gather}\n",
    "    \\frac{\\partial C_{OLS}(\\mathbf{\\beta})}{\\partial \\mathbf{\\beta}} = \\frac{\\partial (\\mathbf{y} - \\mathbf{X}\\mathbf{\\beta})^T(\\mathbf{y} - \\mathbf{X}\\mathbf{\\beta})}{\\partial \\mathbf{\\beta}} = -2(\\mathbf{y} - \\mathbf{X}\\mathbf{\\beta})^T\\mathbf{X}\n",
    "\\end{gather}\n",
    "which gave us the optimal $\\mathbf{\\hat{\\beta}}_{OLS}$ when setting $\\frac{\\partial C_{OLS}}{\\partial \\mathbf{\\beta}}=0$\n",
    "\\begin{gather}\n",
    "    \\mathbf{\\hat{\\beta}}_{OLS} = (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}\n",
    "\\end{gather}\n",
    "\n",
    "For the ridge cost function all we are changing are adding a term $\\lambda |\\mathbf{\\beta}|^2$, so we can use this result when differentiating the new cost function $C_{ridge}=C_{OLS} + \\lambda |\\mathbf{\\beta}|^2$:\n",
    "\\begin{gather}\n",
    "    \\frac{\\partial C_{ridge}(\\mathbf{\\beta})}{\\partial \\mathbf{\\beta}} = \\frac{\\partial C_{OLS}(\\mathbf{\\beta})}{\\partial \\mathbf{\\beta}} + \\frac{\\partial}{\\partial \\mathbf{\\beta}} (\\lambda |\\mathbf{\\beta}|^2)\n",
    "    \\\\ = -2(\\mathbf{y} - \\mathbf{X}\\mathbf{\\beta})^T\\mathbf{X} + \\frac{\\partial}{\\partial \\mathbf{\\beta}} (\\lambda \\sum_{i=0}^{P-1} \\beta_i^2)\n",
    "    \\\\ = 2\\mathbf{\\beta}^T\\mathbf{X}^T\\mathbf{X} - 2\\mathbf{y}^T\\mathbf{X} + \\lambda \\begin{pmatrix} \\frac{\\partial \\sum_{i=0}^{P-1} \\beta_i^2}{\\partial \\beta_0} & ... & \\frac{\\partial \\sum_{i=0}^{P-1} \\beta_i^2}{\\partial \\beta_{P-1}}\\end{pmatrix}\n",
    "    \\\\ = 2\\mathbf{\\beta}^T\\mathbf{X}^T\\mathbf{X} - 2\\mathbf{y}^T\\mathbf{X} + \\lambda \\begin{pmatrix} 2b_0 & ... & 2b_{P-1}\\end{pmatrix} \n",
    "    \\\\ = 2\\mathbf{\\beta}^T\\mathbf{X}^T\\mathbf{X} - 2\\mathbf{y}^T\\mathbf{X} + 2\\lambda \\mathbf{\\beta}^T\n",
    "\\end{gather}\n",
    "We now set the differential equal to zero to minimise, and solve for the optimal $\\mathbf{\\hat{\\beta}}$:\n",
    "\\begin{gather}\n",
    "    2\\mathbf{\\hat{\\beta}}_{ridge}^T\\mathbf{X}^T\\mathbf{X} - 2\\mathbf{y}^T\\mathbf{X} + 2\\lambda \\mathbf{\\hat{\\beta}}_{ridge}^T = 0\n",
    "    \\\\ \\mathbf{\\hat{\\beta}}_{ridge}^T \\mathbf{X}^T\\mathbf{X} + \\lambda \\mathbf{\\hat{\\beta}}_{ridge}^T = \\mathbf{y}^T\\mathbf{X}\n",
    "    \\\\ \\mathbf{\\hat{\\beta}}_{ridge}^T \\left( \\mathbf{X}^T\\mathbf{X} + \\lambda \\mathbf{I} \\right) = \\mathbf{y}^T\\mathbf{X}\n",
    "    \\\\ \\mathbf{\\hat{\\beta}}_{ridge} \\left( \\mathbf{X}^T\\mathbf{X} + \\lambda \\mathbf{I} \\right)^T = \\mathbf{X}^T\\mathbf{y}\n",
    "    \\\\ \\mathbf{\\hat{\\beta}}_{ridge} \\left( \\mathbf{X}^T\\mathbf{X} + \\lambda \\mathbf{I} \\right) = \\mathbf{X}^T\\mathbf{y}\n",
    "    \\\\ \\mathbf{\\hat{\\beta}}_{ridge} = \\left( \\mathbf{X}^T\\mathbf{X} + \\lambda \\mathbf{I} \\right)^{-1} \\mathbf{X}^T\\mathbf{y}\n",
    "\\end{gather}\n",
    "which is the result we wanted to show. We can see that this is exactly what we got from OLS but shrunk by a factor $\\sigma_j^2/(\\sigma_j^2+\\lambda)$. Since $\\lambda>0$, this whole factor approaches 1 when $\\lambda \\rightarrow 0$ and falls to zero when $\\lambda \\rightarrow \\infty$. So we can shrink our $\\mathbf{\\tilde{y}}_{ridge}$ by lowering $\\lambda$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3437861e36b8647e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Ex1b)\n",
    "We put the SVD in the equation:\n",
    "\\begin{gather}\n",
    "    \\mathbf{\\hat{y}}_{OLS} = \\mathbf{X}\\mathbf{\\beta}_{OLS} = \\mathbf{X} (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T\\mathbf{y}\n",
    "    \\\\ = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^T ((\\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^T)^T \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^T)^{-1} (\\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^T)^T \\mathbf{y}\n",
    "    \\\\ = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^T (\\mathbf{V} \\mathbf{\\Sigma}^T \\mathbf{U}^T \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^T)^{-1} \\mathbf{V} \\mathbf{\\Sigma}^T \\mathbf{U}^T \\mathbf{y}\n",
    "\\end{gather}\n",
    "here we use that $\\mathbf{V}^T=\\mathbf{V}^{-1}$ and $\\mathbf{\\Sigma}^T\\mathbf{\\Sigma}=\\begin{pmatrix} \\mathbf{\\tilde{\\Sigma}} \\\\ 0 \\end{pmatrix} \\begin{pmatrix}\\mathbf{\\tilde{\\Sigma}} & 0 \\end{pmatrix} =  \\mathbf{\\tilde{\\Sigma}}^2$, and that\n",
    "\\begin{gather}\n",
    "    \\mathbf{\\tilde{\\Sigma}} = \\begin{pmatrix} \\sigma_0^2 & 0 & ... & 0 \\\\ 0 & \\sigma_1^2 & & 0 \\\\ 0 & 0 & ... & \\sigma_{p-1}^2\\end{pmatrix} \n",
    "\\end{gather}\n",
    "so we get:\n",
    "\\begin{gather}\n",
    "= \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^T (\\mathbf{V} \\mathbf{\\tilde{\\Sigma}}^2 \\mathbf{V}^T)^{-1} \\mathbf{V} \\mathbf{\\Sigma}^T \\mathbf{U}^T \\mathbf{y}\n",
    "\\\\ = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^T \\mathbf{V} \\mathbf{\\tilde{\\Sigma}}^{-2} \\mathbf{V}^T \\mathbf{V} \\mathbf{\\Sigma}^T \\mathbf{U}^T \\mathbf{y}\n",
    "\\\\ = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{\\tilde{\\Sigma}}^{-2} \\mathbf{\\Sigma}^T \\mathbf{U}^T \\mathbf{y}\n",
    "\\end{gather}\n",
    "and here we use $\\mathbf{\\Sigma} \\mathbf{\\tilde{\\Sigma}}^{-2} \\mathbf{\\Sigma}^T=\\mathbf{I}$:\n",
    "\\begin{gather}\n",
    "= \\mathbf{U} \\mathbf{U}^T \\mathbf{y} = \\sum_{j=0}^{p-1}\\mathbf{u}_j \\mathbf{u}_j^T \\mathbf{y}\n",
    "\\end{gather}\n",
    "which is what we wanted to show for the OLS."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we do the same for the Ridge expression:\n",
    "\\begin{gather}\n",
    "    \\mathbf{\\hat{y}}_{Ridge} = \\mathbf{X}\\mathbf{\\beta}_{Ridge}\n",
    "    \\\\ = \\mathbf{X} \\left( \\mathbf{X}^T\\mathbf{X} + \\lambda \\mathbf{I} \\right)^{-1} \\mathbf{X}^T\\mathbf{y}\n",
    "    \\\\ = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^T \\left(\\mathbf{V}\\mathbf{\\tilde{\\Sigma}}^2\\mathbf{V}^T + \\lambda \\mathbf{I} \\right)^{-1} (\\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^T)^T \\mathbf{y}\n",
    "    \\\\ = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^T \\left(\\mathbf{V}\\mathbf{\\tilde{\\Sigma}}^2\\mathbf{V}^T + \\mathbf{V}\\mathbf{V}^T\\lambda \\mathbf{I} \\right)^{-1} \\mathbf{V} \\mathbf{\\Sigma}^T \\mathbf{U}^T \\mathbf{y}\n",
    "    \\\\ = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^T \\left(\\mathbf{V} (\\mathbf{\\tilde{\\Sigma}}^2 + \\lambda \\mathbf{I}) \\mathbf{V}^T \\right)^{-1} \\mathbf{V} \\mathbf{\\Sigma}^T \\mathbf{U}^T \\mathbf{y}\n",
    "    \\\\ = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^T (\\mathbf{V}^T)^{-1} (\\mathbf{\\tilde{\\Sigma}}^2 + \\lambda \\mathbf{I})^{-1} (\\mathbf{V})^{-1} \\mathbf{V} \\mathbf{\\Sigma}^T \\mathbf{U}^T \\mathbf{y}\n",
    "    \\\\ = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^T \\mathbf{V}(\\mathbf{\\tilde{\\Sigma}}^2 + \\lambda \\mathbf{I})^{-1} \\mathbf{V}^T (\\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^T)^T \\mathbf{y}\n",
    "    \\\\ = \\mathbf{U} \\mathbf{\\Sigma} (\\mathbf{\\tilde{\\Sigma}}^2 + \\lambda \\mathbf{I})^{-1} \\mathbf{\\Sigma}^T \\mathbf{U}^T \\mathbf{y}\n",
    "    \\\\ = \\mathbf{U}\\mathbf{U}^T \\frac{ \\mathbf{\\Sigma} \\mathbf{\\Sigma}^T}{\\mathbf{\\tilde{\\Sigma}}^2 + \\lambda \\mathbf{I}} \\mathbf{y}\n",
    "    \\\\ \\mathbf{U}\\mathbf{U}^T \\frac{\\mathbf{\\tilde{\\Sigma}}^2}{\\mathbf{\\tilde{\\Sigma}}^2 + \\lambda \\mathbf{I}} \\mathbf{y}\n",
    "    \\\\ = \\sum_{j=0}^{p-1}\\mathbf{u}_j \\mathbf{u}_j^T \\frac{\\sigma_j^2}{\\sigma_j^2 + \\lambda} \\mathbf{y}\n",
    "\\end{gather}\n",
    "which is our result that we wanted to show."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "76934e6e60bffe45"
  },
  {
   "cell_type": "markdown",
   "id": "2339fe26cc993892",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Exercise 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bbbae8532a0943",
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-09-29T12:02:13.819783300Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def RegressionComparision(x, y, maxdegree, lmbda_vals):\n",
    "    # Identity matrix\n",
    "    I = np.identity(maxdegree)\n",
    "    \n",
    "    # Create design matrix\n",
    "    X = np.zeros((n, maxdegree))\n",
    "    for degree in range(maxdegree):\n",
    "        X[:, degree] = x[:, 0]**(degree+1)  # Exclude the intercept\n",
    "    \n",
    "    # Split into training and test data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "        \n",
    "    # Scale\n",
    "    scaler = StandardScaler()\n",
    "    scaler = scaler.fit(X_train)\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    train_mse_ols = np.zeros(len(lmbda_vals))\n",
    "    test_mse_ols = np.zeros_like(train_mse_ols)\n",
    "    train_mse_rid = np.zeros_like(train_mse_ols)\n",
    "    test_mse_rid = np.zeros_like(train_mse_ols)\n",
    "    for i, lmbda in enumerate(lmbda_vals):\n",
    "        # Linear regression/OLS\n",
    "        beta_ols = np.linalg.inv(X_train_scaled.T @ X_train_scaled) @ X_train_scaled.T @ y_train\n",
    "        y_tilde_ols = X_train_scaled @ beta_ols\n",
    "        y_predict_ols = X_test_scaled @ beta_ols \n",
    "        \n",
    "        # Ridge regression\n",
    "        beta_rid = (np.linalg.inv(X_train_scaled.T @ X_train_scaled + lmbda * I)\n",
    "                @ X_train_scaled.T @ y_train)\n",
    "        y_tilde_rid = X_train_scaled @ beta_rid \n",
    "        y_predict_rid = X_test_scaled @ beta_rid \n",
    "        \n",
    "        # Calculate MSE's\n",
    "        train_mse_ols[i] = mean_squared_error(y_train, y_tilde_ols)\n",
    "        test_mse_ols[i] = mean_squared_error(y_test, y_predict_ols)\n",
    "        train_mse_rid[i] = mean_squared_error(y_train, y_tilde_rid)\n",
    "        test_mse_rid[i] = mean_squared_error(y_test, y_predict_rid)\n",
    "        \n",
    "    # Plot OLS MSE's\n",
    "    plt.plot(lmbda_vals, train_mse_ols, \"k-\", label=\"OLS Train\")\n",
    "    plt.plot(lmbda_vals, test_mse_ols, \"k--\", label=\"OLS Test\")\n",
    "    \n",
    "    # Plot ridge MSE's\n",
    "    plt.plot(lmbda_vals, train_mse_rid, \"r-\", label=\"Ridge Train\")\n",
    "    plt.plot(lmbda_vals, test_mse_rid, \"r--\", label=\"Ridge Test\")\n",
    "    plt.xscale(\"log\")\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.xlabel(\"$\\lambda$\")\n",
    "    plt.ylabel(\"MSE\")\n",
    "    plt.title(f\"Polynomial degree = {maxdegree}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd394bd786065b8",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "n = 10000\n",
    "lmbda_vals = np.asarray([0.0001, 0.001, 0.01, 0.1, 1])\n",
    "\n",
    "# Create random data set\n",
    "x = np.linspace(-3, 3, n).reshape(-1, 1)\n",
    "y = np.exp(-x**2) + 1.5 * np.exp(-(x-2)**2) + np.random.normal(0, 0.1, x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5e1ab1b5a9c275",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Polynomial of degree 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f4bcf2b41ef7c4",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "RegressionComparision(x, y, lmbda_vals=lmbda_vals, maxdegree=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff3d512640423ad",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Polynomial of degree 10:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b578a23bca35389",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "RegressionComparision(x, y, lmbda_vals=lmbda_vals, maxdegree=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5469d65b6d87984",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Polynomial of degree 15:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e09da58fc572cb6",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "RegressionComparision(x, y, lmbda_vals=lmbda_vals, maxdegree=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "With $\\lambda$ plotted logarithmically, we can see that in general the error drops for the Ridge train MSE for every polynomial degree, but it drops more the higher degree we have, meaning we shrink the overfitting Ridge regression. The Ridge test MSE drops extremely more in the higher degree than the lower. For this random dataset it seems the Ridge regression in general is inferior to the ordinary least squares regression $\\it{especially}$ with the higher$\\lambda$-values."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9ba64902e96f840"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
