{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Exercises week 43 and 44\n",
    "### The OR, AND, and XOR gates\n",
    "\n",
    "We have two input values $x_1$ and $x_2$ which decide the output from the two types of gates. Since each input value can be either 0 or 1 we can write the input as a design matrix $X$ where the first and second column represents $x_1$ and $x_2$ respectively as:\n",
    "$$X = \\begin{bmatrix} 0 & 0 \\\\ 0 & 1 \\\\ 1 & 0 \\\\ 1 & 1 \\end{bmatrix}$$\n",
    "\n",
    "The output $y$ for the different gates we can write as the vectors $y^T=[0, 1,1,1]$ for the OR gate, $y^T=[0,0,0,1]$ for the AND gate, and $y^T=[0, 1, 1, 0]$ for the XOR gate. We setup this matrix and these vectors:\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bf257eb40befa320"
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "from jax import grad\n",
    "\n",
    "# Set up design matrix and output vectors\n",
    "X = np.asarray([\n",
    "    [0, 0],\n",
    "    [0, 1],\n",
    "    [1, 0],\n",
    "    [1, 1]\n",
    "])\n",
    "\n",
    "# Gate target arrays\n",
    "yOR = np.asarray([0, 1, 1, 1])\n",
    "yAND = np.asarray([0, 0, 0, 1])\n",
    "yXOR = np.asarray([0, 1, 1, 0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-24T20:20:55.537896400Z",
     "start_time": "2023-10-24T20:20:55.510744400Z"
    }
   },
   "id": "58160402d5f28b1c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We create our NN architecture with the Sigmoid function $\\sigma$ as activation function where\n",
    "\\begin{equation}\n",
    "    \\sigma(x) = \\frac{1}{1+e^{-x}}\n",
    "\\end{equation}\n",
    "as such"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "40a26ec520420a39"
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.60172129  1.15161897]\n",
      " [-1.35946236  0.22205533]]\n",
      "[[-0.77586755  0.8087058 ]\n",
      " [-0.19862826 -1.57869386]]\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "n_hidden_nodes = 2  # hidden nodes per layer\n",
    "n_categories = 2  # output value categories, for gates we only find 0 or 1\n",
    "n_inputs, n_features = X.shape  # 2 inputs, 4 features\n",
    "target_gate = \"OR\"  # choose which target gate to train on\n",
    "\n",
    "# Activation function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "# Initialize random number generator with seed\n",
    "rng = np.random.default_rng(2023)\n",
    "\n",
    "# Weights and bias in the hidden layer\n",
    "hidden_weights = rng.standard_normal((n_features, n_hidden_nodes))  # weights normally distributed\n",
    "hidden_bias = np.zeros(n_hidden_nodes) + 0.01\n",
    "\n",
    "# Weights and bias in the output layer\n",
    "output_weights = rng.standard_normal((n_hidden_nodes, n_categories))  # weights normally distributed\n",
    "output_bias = np.zeros(n_hidden_nodes) + 0.01\n",
    "\n",
    "print(hidden_weights)\n",
    "print(output_weights)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-24T20:20:55.539890900Z",
     "start_time": "2023-10-24T20:20:55.516543700Z"
    }
   },
   "id": "cfd0e5879201f65e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Feed forward\n",
    "Then we set up the feed forward algorithm and compare one pass with the target vectors $y^T$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "90be2689e40d3f08"
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets:\n",
      "yOR = [0 1 1 1]\n",
      "yAND = [0 0 0 1]\n",
      "yXOR = [0 1 1 0]\n",
      "\n",
      "Prediction:\n",
      "[1. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "def feed_forward(X):\n",
    "    \"\"\"Feed forward algorithm for one hidden layer\"\"\"\n",
    "    # weighted sum of inputs to the hidden layer\n",
    "    z_h = X @ hidden_weights + hidden_bias\n",
    "\n",
    "    # activation in the hidden layer\n",
    "    a_h = sigmoid(z_h)\n",
    "\n",
    "    # weighted sum of inputs to the output layer\n",
    "    z_o = a_h @ output_weights + output_bias\n",
    "\n",
    "    # axis 0 holds each input and axis 1 the probabilities of each category\n",
    "    probabilities = sigmoid(z_o)\n",
    "    return a_h, probabilities\n",
    "\n",
    "\n",
    "def predict(X):\n",
    "    \"\"\"Get neural network prediction using the feed forward algorithm.\"\"\"\n",
    "    probabilities = feed_forward(X)[1]\n",
    "    return np.argmax(probabilities, axis=1).astype(float)  # also convert to float to not upset Jax\n",
    "\n",
    "\n",
    "# Make prediction and compare with gate target y_vectors\n",
    "predictions = predict(X)\n",
    "\n",
    "print(\"Targets:\")\n",
    "print(\"yOR =\", yOR)\n",
    "print(\"yAND =\", yAND)\n",
    "print(\"yXOR =\", yXOR)\n",
    "\n",
    "print(\"\\nPrediction:\")\n",
    "print(predictions)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-24T20:20:55.609995Z",
     "start_time": "2023-10-24T20:20:55.524441400Z"
    }
   },
   "id": "197d2b7a7bd3be38"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We see this prediction does not match any target. This is because we only did one pass and that was with random starting weights. Now we setup the cost function and the back propagation algorithm.\n",
    "\n",
    "<!--- \n",
    "For the cost function we use the cross entropy for binary classification given as\n",
    "$$C(\\boldsymbol{\\theta}) = -\\sum_{i=1}^n \\left( y_i \\ln [p(y_i | x_i, \\boldsymbol{\\theta)}] + (1-y_i)\\ln[1-p(y_i | x_i, \\boldsymbol{\\theta)}] \\right)$$\n",
    "\n",
    "where the probabilities $p$ we have from the sigmoid function \n",
    "$$p(y_i=1|x_i, \\boldsymbol{\\theta}) = \\frac{\\exp(\\theta_0 + \\theta_1 x_i)}{1- \\exp(\\theta_0 + \\theta_1 x_i)}$$\n",
    "$$p(y_i=0|x_i, \\boldsymbol{\\theta}) = 1 - p(y_i=1|x_i, \\boldsymbol{\\theta})$$ \n",
    "-->\n",
    "\n",
    "For the cost function we use the cross entropy for binary classification given as\n",
    "$$C(\\boldsymbol{W}) = -\\sum_{i=1}^n \\left( t_i \\log a_i^L + (1-t_i)\\log(1-a_i^L) \\right)$$\n",
    "where $t$ is the target and $a^L$ is the final activation from the final/output layer."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b24a0a554fb220f"
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "if target_gate == \"OR\":\n",
    "    target = np.asarray([0, 1, 1, 1])\n",
    "elif target_gate == \"AND\":\n",
    "    target = np.asarray([0, 0, 0, 1])\n",
    "elif target_gate == \"XOR\":\n",
    "    target = np.asarray([0, 1, 1, 0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-24T20:20:55.620949800Z",
     "start_time": "2023-10-24T20:20:55.530881900Z"
    }
   },
   "id": "ea652999d4738c39"
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.54245\n"
     ]
    }
   ],
   "source": [
    "def cost_cross_entropy(target):\n",
    "    \"\"\"Returns a function for the logistic cross entropy for binary classification / log loss function using a given target vector.\"\"\"\n",
    "    d = 1e-9  # small value to avoid infinities\n",
    "    def func(X):\n",
    "        return -(1 / target.size) * jnp.sum(target * jnp.log(X + d))\n",
    "    # def func(x):\n",
    "    #     return -np.sum(target * jnp.log(x + d) + (1 - target) * jnp.log(1 - x + d))\n",
    "    return func\n",
    "\n",
    "\n",
    "cost_func = cost_cross_entropy(target)\n",
    "print(cost_func(predictions))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-24T20:20:55.621946600Z",
     "start_time": "2023-10-24T20:20:55.536899Z"
    }
   },
   "id": "e46a022e02ed29b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Calculating the analytical gradients for back propagation\n",
    "We differentiate the cost function with regards to (wgt) the activation of the output layer $a_i^L$ and get:\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial C}{\\partial a_i^L} &= -\\frac{\\partial}{\\partial a_i^L}(t_i \\ln(a_i^L) + (1-t_i)\\ln(1-a_i^L))\n",
    "    \\\\ &= -(\\frac{t_i}{a_i^L} + \\frac{1-t_i}{1-a_i^L}(-1))\n",
    "    \\\\ &= \\frac{1-t_i}{1-a_i^L} - \\frac{t_i}{a_i^L}\n",
    "    \\\\ &= \\frac{a_i^L(1-t_i)}{a_i^L(1-a_i^L)} - \\frac{t_i(1-a_i^L)}{a_i^L(1-a_i^L)}\n",
    "    \\\\ &= \\underline{\\frac{a_i^L-t_i}{a_i^L(1-a_i^L)}}\n",
    "\\end{align*}\n",
    "\n",
    "The expression for the output error $\\delta^L$ is \n",
    "\n",
    "\\begin{equation}\n",
    "    \\delta_i^L = \\sigma'(z_i^L)\\frac{\\partial C}{\\partial a_i^L}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\sigma$ is our Sigmoid function, we can write it as\n",
    "\n",
    "\\begin{equation}\n",
    "    \\sigma'(x) = \\frac{e^{-x}}{(1+e^{-x})^2}\n",
    "\\end{equation}"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "318e19592a808453"
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "def dsigmoid_dx(x):\n",
    "    \"\"\"Derivative of sigmoid function\"\"\"\n",
    "    return np.exp(-x)/(1 + np.exp(-x))**2\n",
    "\n",
    "def analytic_gradient(target):\n",
    "    d = 1-9  # small value to avoid infinities\n",
    "    def func(x):\n",
    "        return dsigmoid_dx(x) * (x - target)/(x * (1 - x) + d)\n",
    "    return func\n",
    "\n",
    "analy_grad = analytic_gradient(target)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-24T20:20:55.623941600Z",
     "start_time": "2023-10-24T20:20:55.544684400Z"
    }
   },
   "id": "9a0522f0ec9e0982"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Using automatic differentiation for the gradient"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "129570a9ac436d4c"
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.02457649  0.03125     0.03125     0.03125   ]\n",
      "[-0.0e+00 -2.5e+08 -2.5e+08 -2.5e+08]\n"
     ]
    }
   ],
   "source": [
    "auto_grad = grad(cost_func)\n",
    "\n",
    "# Compare analytical vs automatic with yOR\n",
    "print(analy_grad(predictions))\n",
    "print(auto_grad(predictions))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-24T20:20:55.624938700Z",
     "start_time": "2023-10-24T20:20:55.551181300Z"
    }
   },
   "id": "6a50c579cef65872"
  },
  {
   "cell_type": "markdown",
   "source": [
    "These should be the same, something must be wrong with my analytical expression as the first one is not zero."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6662fe051bfacfe0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Back propagation\n",
    "I choose to use automatic differentiation here, something seems to be off about my analytical gradient function.\n",
    "\n",
    "Since we only have one hidden layer we need to propogate only once from the output layer to the first hidden layer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5dc14ab235c1c487"
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4,)\n",
      "(2, 2)\n",
      "[[-0.77586755  0.8087058 ]\n",
      " [-0.19862826 -1.57869386]]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "dot_general requires contracting dimensions to have the same shape, got (4,) and (2,).",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[79], line 32\u001B[0m\n\u001B[0;32m     29\u001B[0m     hidden_bias_change \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39msum(error_hidden, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m     30\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[1;32m---> 32\u001B[0m \u001B[43mback_propagate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[79], line 19\u001B[0m, in \u001B[0;36mback_propagate\u001B[1;34m(X)\u001B[0m\n\u001B[0;32m     16\u001B[0m \u001B[38;5;28mprint\u001B[39m(output_weights)\n\u001B[0;32m     18\u001B[0m \u001B[38;5;66;03m# Gradient for only hidden layer delta^1\u001B[39;00m\n\u001B[1;32m---> 19\u001B[0m grad_h \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39msum(\u001B[43mgrad_o\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m@\u001B[39;49m\u001B[43m \u001B[49m\u001B[43moutput_weights\u001B[49m \u001B[38;5;241m*\u001B[39m dsigmoid_dx(z_h))\n\u001B[0;32m     21\u001B[0m \u001B[38;5;28mprint\u001B[39m(grad_h\u001B[38;5;241m.\u001B[39mshape)\n\u001B[0;32m     23\u001B[0m \u001B[38;5;66;03m# Changes for the output layer\u001B[39;00m\n",
      "File \u001B[1;32mE:\\Programmer\\Python\\Python311\\Lib\\site-packages\\jax\\_src\\numpy\\array_methods.py:256\u001B[0m, in \u001B[0;36m_defer_to_unrecognized_arg.<locals>.deferring_binary_op\u001B[1;34m(self, other)\u001B[0m\n\u001B[0;32m    254\u001B[0m args \u001B[38;5;241m=\u001B[39m (other, \u001B[38;5;28mself\u001B[39m) \u001B[38;5;28;01mif\u001B[39;00m swap \u001B[38;5;28;01melse\u001B[39;00m (\u001B[38;5;28mself\u001B[39m, other)\n\u001B[0;32m    255\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(other, _accepted_binop_types):\n\u001B[1;32m--> 256\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mbinary_op\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    257\u001B[0m \u001B[38;5;66;03m# Note: don't use isinstance here, because we don't want to raise for\u001B[39;00m\n\u001B[0;32m    258\u001B[0m \u001B[38;5;66;03m# subclasses, e.g. NamedTuple objects that may override operators.\u001B[39;00m\n\u001B[0;32m    259\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(other) \u001B[38;5;129;01min\u001B[39;00m _rejected_binop_types:\n",
      "    \u001B[1;31m[... skipping hidden 12 frame]\u001B[0m\n",
      "File \u001B[1;32mE:\\Programmer\\Python\\Python311\\Lib\\site-packages\\jax\\_src\\numpy\\lax_numpy.py:3192\u001B[0m, in \u001B[0;36mmatmul\u001B[1;34m(a, b, precision, preferred_element_type)\u001B[0m\n\u001B[0;32m   3190\u001B[0m a \u001B[38;5;241m=\u001B[39m lax\u001B[38;5;241m.\u001B[39msqueeze(a, \u001B[38;5;28mtuple\u001B[39m(a_squeeze))\n\u001B[0;32m   3191\u001B[0m b \u001B[38;5;241m=\u001B[39m lax\u001B[38;5;241m.\u001B[39msqueeze(b, \u001B[38;5;28mtuple\u001B[39m(b_squeeze))\n\u001B[1;32m-> 3192\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[43mlax\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdot_general\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   3193\u001B[0m \u001B[43m  \u001B[49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mb\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43m(\u001B[49m\u001B[43m(\u001B[49m\u001B[43mndim\u001B[49m\u001B[43m(\u001B[49m\u001B[43ma\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mndim\u001B[49m\u001B[43m(\u001B[49m\u001B[43mb\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mb_is_mat\u001B[49m\u001B[43m,\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43ma_batch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mb_batch\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3194\u001B[0m \u001B[43m  \u001B[49m\u001B[43mprecision\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprecision\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpreferred_element_type\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpreferred_element_type\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3195\u001B[0m result \u001B[38;5;241m=\u001B[39m lax\u001B[38;5;241m.\u001B[39mtranspose(out, perm)\n\u001B[0;32m   3196\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m lax_internal\u001B[38;5;241m.\u001B[39m_convert_element_type(result, preferred_element_type, output_weak_type)\n",
      "    \u001B[1;31m[... skipping hidden 7 frame]\u001B[0m\n",
      "File \u001B[1;32mE:\\Programmer\\Python\\Python311\\Lib\\site-packages\\jax\\_src\\lax\\lax.py:2558\u001B[0m, in \u001B[0;36m_dot_general_shape_rule\u001B[1;34m(lhs, rhs, dimension_numbers, precision, preferred_element_type)\u001B[0m\n\u001B[0;32m   2555\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m core\u001B[38;5;241m.\u001B[39mdefinitely_equal_shape(lhs_contracting_shape, rhs_contracting_shape):\n\u001B[0;32m   2556\u001B[0m   msg \u001B[38;5;241m=\u001B[39m (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdot_general requires contracting dimensions to have the same \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   2557\u001B[0m          \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mshape, got \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m and \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m-> 2558\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(msg\u001B[38;5;241m.\u001B[39mformat(lhs_contracting_shape, rhs_contracting_shape))\n\u001B[0;32m   2560\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _dot_general_shape_computation(lhs\u001B[38;5;241m.\u001B[39mshape, rhs\u001B[38;5;241m.\u001B[39mshape, dimension_numbers)\n",
      "\u001B[1;31mTypeError\u001B[0m: dot_general requires contracting dimensions to have the same shape, got (4,) and (2,)."
     ]
    }
   ],
   "source": [
    "# Choose gradient method\n",
    "gradient_func = auto_grad\n",
    "\n",
    "def back_propagate(X):\n",
    "    \"\"\"Back propagation algorithm for one hidden layer\"\"\"\n",
    "    a_h, probabilities = feed_forward(X)\n",
    "    \n",
    "    # Hidden layer \n",
    "    z_h = X @ hidden_weights + hidden_bias\n",
    "    \n",
    "    # Gradient for the output layer delta^L:\n",
    "    grad_o = gradient_func(predictions)\n",
    "    \n",
    "    print(grad_o.shape)\n",
    "    print(output_weights.shape)\n",
    "    print(output_weights)\n",
    "\n",
    "    # Gradient for only hidden layer delta^1\n",
    "    grad_h = np.sum(grad_o @ output_weights * dsigmoid_dx(z_h))\n",
    "    \n",
    "    print(grad_h.shape)\n",
    "    \n",
    "    # Changes for the output layer\n",
    "    output_weights_change = a_h.T @ grad_o \n",
    "    output_bias_change = np.sum(error_output, axis=0)\n",
    "    \n",
    "    # Changes for the hidden layer\n",
    "    hidden_weights_change = np.matmul(X.T, error_hidden)\n",
    "    hidden_bias_change = np.sum(error_hidden, axis=0)\n",
    "    return\n",
    "\n",
    "back_propagate(X)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-24T21:00:03.451182800Z",
     "start_time": "2023-10-24T21:00:03.356593800Z"
    }
   },
   "id": "5bb7c334b5bfe1cc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Iterating until convergence"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1e514c3db4789bba"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "eta = 0.5  # learning rate"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "54cbc0df5b4c8b9c"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "4e2b2f706db7f707"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
