{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Exercises week 43 and 44\n",
    "### The OR, AND, and XOR gates\n",
    "\n",
    "We have two input values $x_1$ and $x_2$ which decide the output from the two types of gates. Since each input value can be either 0 or 1 we can write the input as a design matrix $X$ where the first and second column represents $x_1$ and $x_2$ respectively as:\n",
    "$$X = \\begin{bmatrix} 0 & 0 \\\\ 0 & 1 \\\\ 1 & 0 \\\\ 1 & 1 \\end{bmatrix}$$\n",
    "\n",
    "The output $y$ for the different gates we can write as the vectors $y^T=[0, 1,1,1]$ for the OR gate, $y^T=[0,0,0,1]$ for the AND gate, and $y^T=[0, 1, 1, 0]$ for the XOR gate. We setup this matrix and these vectors:\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bf257eb40befa320"
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns  # heatmap plots\n",
    "import jax.numpy as jnp\n",
    "from jax import grad  # automatic differentiation\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Set up design matrix and output vectors\n",
    "X = np.asarray([\n",
    "    [0, 0],\n",
    "    [0, 1],\n",
    "    [1, 0],\n",
    "    [1, 1]\n",
    "])\n",
    "\n",
    "# Gate target arrays\n",
    "yOR = np.asarray([0, 1, 1, 1]).reshape(-1, 1)\n",
    "yAND = np.asarray([0, 0, 0, 1]).reshape(-1, 1)\n",
    "yXOR = np.asarray([0, 1, 1, 0]).reshape(-1, 1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-26T17:36:47.197313800Z",
     "start_time": "2023-10-26T17:36:47.087382800Z"
    }
   },
   "id": "58160402d5f28b1c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We create our NN architecture with the Sigmoid function $\\sigma$ as activation function where\n",
    "\\begin{equation}\n",
    "    \\sigma(x) = \\frac{1}{1+e^{-x}}\n",
    "\\end{equation}\n",
    "as such"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "40a26ec520420a39"
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before back propagation:\n",
      "\n",
      "Output weights and bias:\n",
      "[[-0.77586755]\n",
      " [ 0.8087058 ]]\n",
      "[0.01]\n",
      "\n",
      "Hidden weights and bias:\n",
      "[[ 0.60172129  1.15161897]\n",
      " [-1.35946236  0.22205533]]\n",
      "[0.01 0.01]\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "n_hidden_nodes = 2  # hidden nodes per layer\n",
    "n_categories = 1  # output nodes\n",
    "n_inputs, n_features = X.shape  # 2 inputs, 4 features\n",
    "target_gate = \"XOR\"  # choose which target gate to train on\n",
    "\n",
    "\n",
    "# Activation function \n",
    "def activation(x):\n",
    "    # Use sigmoid function:\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "# Initialize random number generator with seed\n",
    "rng = np.random.default_rng(2023)\n",
    "\n",
    "# Weights and bias in the hidden layer\n",
    "hidden_weights = rng.standard_normal((n_features, n_hidden_nodes))  # weights normally distributed\n",
    "hidden_bias = np.zeros(n_hidden_nodes) + 0.01\n",
    "\n",
    "# Weights and bias in the output layer\n",
    "output_weights = rng.standard_normal((n_hidden_nodes, n_categories))  # weights normally distributed\n",
    "output_bias = np.zeros(n_categories) + 0.01\n",
    "\n",
    "print(\"Before back propagation:\\n\")\n",
    "print(\"Output weights and bias:\")\n",
    "print(output_weights)\n",
    "print(output_bias)\n",
    "\n",
    "print()\n",
    "print(\"Hidden weights and bias:\")\n",
    "print(hidden_weights)\n",
    "print(hidden_bias)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-26T17:36:47.225238100Z",
     "start_time": "2023-10-26T17:36:47.092280200Z"
    }
   },
   "id": "cfd0e5879201f65e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Feed forward\n",
    "Then we set up the feed forward algorithm and compare one pass with the target vectors $y^T$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "90be2689e40d3f08"
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "outputs": [],
   "source": [
    "# Get the chosen target array\n",
    "if target_gate == \"OR\":\n",
    "    target = yOR\n",
    "elif target_gate == \"AND\":\n",
    "    target = yAND\n",
    "elif target_gate == \"XOR\":\n",
    "    target = yXOR\n",
    "else:\n",
    "    raise ValueError(f\"Target gate not found, got '{target_gate}'.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-26T17:36:47.228230100Z",
     "start_time": "2023-10-26T17:36:47.100060Z"
    }
   },
   "id": "56fac192b81a286c"
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target:\n",
      "[[0]\n",
      " [1]\n",
      " [1]\n",
      " [0]]\n",
      "\n",
      "Probabilities:\n",
      "[[0.50662492]\n",
      " [0.5747513 ]\n",
      " [0.53068917]\n",
      " [0.60044713]]\n",
      "\n",
      "Prediction:\n",
      "[[1]\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "def feed_forward(X):\n",
    "    \"\"\"Feed forward algorithm for one hidden layer\"\"\"\n",
    "    # Weighted sum of inputs to the hidden layer\n",
    "    z_h = X @ hidden_weights + hidden_bias\n",
    "\n",
    "    # Activation in the hidden layer\n",
    "    a_h = activation(z_h)\n",
    "\n",
    "    # Weighted sum of inputs to the output layer\n",
    "    z_o = a_h @ output_weights + output_bias\n",
    "\n",
    "    # Activation of output layer; contains the output probabilities\n",
    "    probabilities = activation(z_o)  # this is a_o\n",
    "    return a_h, probabilities\n",
    "\n",
    "\n",
    "@np.vectorize\n",
    "def predict(probability):\n",
    "    \"\"\"Get prediction from array with floats. Step function.\"\"\"\n",
    "    if not isinstance(probability, (int, float)):\n",
    "        raise ValueError(f\"Probability must be a number, int or float. Got {type(probability)}\")\n",
    "    if probability < 0.5:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "\n",
    "# Make prediction and compare with gate target y_vectors\n",
    "a_h, probabilities = feed_forward(X)\n",
    "predictions = predict(probabilities)\n",
    "\n",
    "print(\"Target:\")\n",
    "print(target)\n",
    "\n",
    "print(\"\\nProbabilities:\")\n",
    "print(probabilities)\n",
    "\n",
    "print(\"\\nPrediction:\")\n",
    "print(predictions)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-26T17:36:47.328977Z",
     "start_time": "2023-10-26T17:36:47.105744400Z"
    }
   },
   "id": "197d2b7a7bd3be38"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We see this prediction does not match any target. This is because we only did one pass and that was with random starting weights. Now we setup the cost function and the back propagation algorithm.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fb88c7ba8f6ff0d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<!-- \n",
    "For the cost function we use the cross entropy for binary classification given as\n",
    "$$C(\\boldsymbol{\\theta}) = -\\sum_{i=1}^n \\left( y_i \\ln [p(y_i | x_i, \\boldsymbol{\\theta)}] + (1-y_i)\\ln[1-p(y_i | x_i, \\boldsymbol{\\theta)}] \\right)$$\n",
    "\n",
    "where the probabilities $p$ we have from the sigmoid function \n",
    "$$p(y_i=1|x_i, \\boldsymbol{\\theta}) = \\frac{\\exp(\\theta_0 + \\theta_1 x_i)}{1- \\exp(\\theta_0 + \\theta_1 x_i)}$$\n",
    "$$p(y_i=0|x_i, \\boldsymbol{\\theta}) = 1 - p(y_i=1|x_i, \\boldsymbol{\\theta})$$ \n",
    "-->\n",
    "### Cost function\n",
    "\n",
    "For the cost function we use the cross entropy for binary classification given as\n",
    "\\begin{equation}\n",
    "    C(\\boldsymbol{W}) = -\\sum_{i=1}^n \\left( t_i \\log a_i^L + (1-t_i)\\log(1-a_i^L) \\right)\n",
    "\\end{equation}\n",
    "\n",
    "where $t$ is the target and $a^L$ is the final activation from the final/output layer, expressed for a general layer $l$ as:\n",
    "\\begin{equation*}\n",
    "    a_i^l = \\sigma(z_i^l)\n",
    "\\end{equation*}\n",
    "\n",
    "and $z^L$ is the weighted sum of inputs to the output layer, expressed for a general layer $l$ as:\n",
    "\n",
    "\\begin{equation*}\n",
    "    z_i^l = \\sum_j w_{ij}^l a_j^{l-1} + b_i^l\n",
    "\\end{equation*}\n",
    "\n",
    "or in matrix notification:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathbf{a}^l = \\sigma(\\mathbf{z}^l)\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathbf{z}^l = \\mathbf{W}^l \\mathbf{a}^{l-1} + \\mathbf{b}^l\n",
    "\\end{equation}\n",
    "\n",
    "for the first layer $l=1$ we have the input layer $a^0$ which is just the input design matrix $X$:\n",
    "\\begin{equation}\n",
    "    \\mathbf{z}^{l=1} = \\mathbf{W}^{l=1} \\mathbf{X} + \\mathbf{b}^{l=1}\n",
    "\\end{equation}"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b24a0a554fb220f"
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "outputs": [
    {
     "data": {
      "text/plain": "Array(41.446533, dtype=float32)"
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cost_log_reg(target):\n",
    "    \"\"\"Returns a function for the logistic cross entropy for binary classification / log loss function created using a given target vector.\"\"\"\n",
    "    d = 1e-9  # small value to avoid infinities\n",
    "\n",
    "    def func(a):\n",
    "        return -jnp.sum(\n",
    "                (target * jnp.log(a + d)) + ((1 - target) * jnp.log(1 - a + d))\n",
    "        )\n",
    "\n",
    "    return func\n",
    "\n",
    "\n",
    "cost_func = cost_log_reg(target)\n",
    "cost_func(predictions)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-26T17:36:47.331968900Z",
     "start_time": "2023-10-26T17:36:47.111634800Z"
    }
   },
   "id": "e46a022e02ed29b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Calculating the analytical gradients for back propagation\n",
    "\n",
    "To find the change/gradient for the values we need to update by doing back propagation we need to differentiate our cost function with regards to what we want to update. Firstly we will update the weights, and since we want to start at the output layer and move backwards into the hidden layers we start by differentiate with regards to the weights in the output layer $w^L$. Using the chain rule we can start to write it as \n",
    "\n",
    "\\begin{align*}\n",
    "    \\nabla w_{ij}^L = \\frac{\\partial C}{\\partial w^L_{ij}} &= \\frac{\\partial C}{\\partial a^L_i} \\frac{\\partial a^L_i}{\\partial w^L_{ij}}\n",
    "    \\\\ &= \\frac{\\partial C}{\\partial a^L_i} \\frac{\\partial a^L_i}{\\partial z^L_{i}} \\frac{\\partial z^L_{i}}{\\partial w^L_{ij}}\n",
    "\\end{align*}\n",
    "\n",
    "We begin by the first derivate by differentiating the cost function as defined above:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial C}{\\partial a_i^L} &= -\\frac{\\partial}{\\partial a_i^L}(t_i\\ln(a_i^L) + (1-t_i)\\ln(1-a_i^L))\n",
    "    \\\\ &= -(\\frac{t_i}{a_i^L} + \\frac{1-t_i}{1-a_i^L}(-1))\n",
    "    \\\\ &= \\frac{1-t_i}{1-a_i^L} - \\frac{t_i}{a_i^L}\n",
    "    \\\\ &= \\frac{a_i^L(1-t_i)}{a_i^L(1-a_i^L)} - \\frac{t_i(1-a_i^L)}{a_i^L(1-a_i^L)}\n",
    "    \\\\ &= \\underline{\\frac{a_i^L-t_i}{a_i^L(1-a_i^L)}}\n",
    "\\end{align*}\n",
    "\n",
    "for the second derivate we differentiate $a^L_i=\\sigma(z_i^L)$ and get\n",
    "\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial a^L_i}{\\partial z^L_{i}} &= \\sigma'(z_i^L)\n",
    "    \\\\ &= \\frac{\\partial}{\\partial z^L_{i}} \\left( \\frac{1}{1+\\exp(-z_i^L)} \\right)\n",
    "    \\\\ &= \\frac{0-[-\\exp(-z_i^L)] }{(1+\\exp(-z_i^L))^2}\n",
    "    \\\\ &= \\frac{\\exp({-z_i^L})}{(1+\\exp({-z_i^L}))^2}\n",
    "\\end{align*}\n",
    "\n",
    "this can be rewritten more nicely by again using the above expression for $a_i^L=\\frac{1}{1+\\exp(-z_i^L)}$, which gives $\\exp(-z_i^L)=\\frac{1}{a_i^L} - 1$. We can substitute the denominator with $a_i^L$ and the numerator with $\\frac{1}{a_i^L} - 1$ and get\n",
    "\n",
    "\\begin{align*}\n",
    "    &= (\\frac{1}{a_i^L} - 1)(a_i^L)^2\n",
    "    \\\\ &= \\underline{a_i^L (1-a_i^L)}\n",
    "\\end{align*}\n",
    "\n",
    "for the third derivate we differentiate $z^L_i = \\sum_j \\left( w^L_{ij}a^{L-1}_j + b^L_i\\right)$ and get\n",
    "\n",
    "\\begin{align*}\n",
    "   \\frac{\\partial z^L_{i}}{\\partial w^L_{ij}} &= \\frac{\\partial}{\\partial w^L_{ij}} \\left( \\sum_j w^L_{ij}a^{L-1}_j + b^L_i \\right)\n",
    "   \\\\ &= \\underline{a^{L-1}_j}\n",
    "\\end{align*}\n",
    "\n",
    "so our final expression for the gradient is\n",
    "\n",
    "\\begin{align*}\n",
    "    \\nabla w_{ij}^L  &= \\frac{a_i^L-t_i}{a_i^L(1-a_i^L)} a_i^L (1-a_i^L) a^{L-1}_j\n",
    "    \\\\ &= a^{L-1}_j (a_i^L-t_i)\n",
    "\\end{align*}\n",
    "\n",
    "we choose to defined the second part as the output error:\n",
    "\\begin{equation*}  \n",
    "    \\delta_i^L \\equiv  \\frac{\\partial C}{z_i^L} = \\frac{\\partial C}{\\partial a^L_i} \\frac{\\partial a^L_i}{\\partial z^L_{i}} =   a_i^L-t_i\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "\n",
    "or using matrix notification we can write these two equations as\n",
    "\n",
    "\\begin{equation}\n",
    "    \\boldsymbol{\\delta}^L = \\mathbf{a}^L-\\mathbf{t}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    \\nabla \\mathbf{W}^L = (\\mathbf{a}^{L-1})^T \\boldsymbol{\\delta}^L\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<!-- OLD SHIT:\n",
    "The gradients are\n",
    "\\begin{equation}\n",
    "    \\nabla W_L=a_{L-1}^T \\delta_L  = a_{h}^T \\delta_L \n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "\\end{equation}\n",
    "but for our only hidden layer we have\n",
    "\\begin{equation}\n",
    "    \\nabla W_h = X^T\\delta_h\n",
    "\\end{equation}\n",
    "and the gradients for the biases will be\n",
    "\\begin{align}\n",
    "    \\nabla b_L &= \\sum_i^{n_{inputs}} \\delta_L\n",
    "    \\\\ \\nabla b_h &= \\sum_i^{n_{inputs}} \\delta_h\n",
    "\\end{align}\n",
    "\n",
    "The expression we need to now calculate is  the expression for the output error $\\delta_L$:\n",
    "\n",
    "We differentiate the cost function with regards to the activation of the output layer $a_L$ and get:\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial C}{\\partial a_L} &= -\\frac{\\partial}{\\partial a_L}(t \\ln(a_L) + (1-t)\\ln(1-a_L))\n",
    "    \\\\ &= -(\\frac{t_}{a_L} + \\frac{1-t_}{1-a_L}(-1))\n",
    "    \\\\ &= \\frac{1-t_}{1-a_L} - \\frac{t_}{a_L}\n",
    "    \\\\ &= \\frac{a_L(1-t_)}{a_L(1-a_L)} - \\frac{t_(1-a_L)}{a_L(1-a_L)}\n",
    "    \\\\ &= \\underline{\\frac{a_L-t_}{a_L(1-a_L)}}\n",
    "\\end{align*}\n",
    "\n",
    "The expression for the output error $\\delta_L$ is \n",
    "\n",
    "\\begin{equation}\n",
    "    \\delta_i^L = \\sigma'(z_i^L)\\frac{\\partial C}{\\partial a_i^L}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\sigma$ is our Sigmoid function, we can differentiate it to be\n",
    "\n",
    "\\begin{equation}\n",
    "    \\sigma'(x) = \\frac{e^{-x}}{(1+e^{-x})^2}\n",
    "\\end{equation}\n",
    "which gives the following when putting in $a_i^L=\\sigma(a_i^L)=1/(1+e^{-z_i^L})$:\n",
    "\\begin{gather*}\n",
    "    \\sigma'(z_i^L) &= \\frac{e^{-z_i^L}}{(1+e^{-z_i^L})^2}\n",
    "    \\\\ &= \\left( \\frac{1}{a_i^L} - 1 \\right) (a_i^L)^2\n",
    "    \\\\ &= \\underline{a_i^L(1-a_i^L)}\n",
    "\\end{gather*}\n",
    "\n",
    "which gives \n",
    "\\begin{align*}\n",
    "    \\delta_L &= a_L(1-a_L)\\ \\frac{a_L-t}{a_L(1-a_L)}\n",
    "    \\\\ &= \\underline{a_L-t}\n",
    "\\end{align*}\n",
    "-->\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "318e19592a808453"
  },
  {
   "cell_type": "markdown",
   "source": [
    "For the general layer $l$ error $\\delta^l$ we want to write it as a function of the previous layer's $\\delta^{l+1}$ so we can back propagate beginning from the output layer. By replacing $L$ with $l$ and using the chain rule and summing over all entries $j$ we can write this out as\n",
    "\\begin{align*}\n",
    "    \\delta_i^l &= \\frac{\\partial C}{\\partial z_i^l} \n",
    "    \\\\ &= \\sum_{j} \\frac{\\partial C}{\\partial z_j^{l+1}} \\frac{\\partial z_j^{l+1}}{\\partial z_i^l}\n",
    "    \\\\ &= \\sum_{j} \\delta_j^{l+1}\\frac{\\partial z_j^{l+1}}{\\partial z_i^l}\n",
    "\\end{align*}\n",
    "\n",
    "for this derivative we again use our definition which gives $z_i^{l+1} = \\sum_j w_{ij}^{l+1}a_j^l + b_i^{l+1}$, therefore we get\n",
    "\\begin{align*}\n",
    "     &= \\sum_{j} \\delta_j^{l+1} w_{ij}^{l+1} \\frac{\\partial a_j^l}{\\partial z_i^l}\n",
    "     \\\\ &= \\sum_{j} \\delta_j^{l+1} w_{ij}^{l+1} \\sigma'(z_i^l)\n",
    "     \\\\ &= \\underline{\\sum_{j} \\delta_j^{l+1} w_{ij}^{l+1} a_i^l (1-a_i^l)}\n",
    "\\end{align*}\n",
    "\n",
    "or in matrix notation as \n",
    "\\begin{equation}\n",
    "    \\boldsymbol{\\delta}^l = \\boldsymbol{\\delta}^{l+1} (\\mathbf{W}^{l+1})^T \\odot a^l \\odot (1-a^l)\n",
    "\\end{equation}\n",
    "\n",
    "where $\\odot$ is the $\\textit{Hadamard product}$ which means element-wise multiplication.\n",
    "\n",
    "The weights gradient will then be\n",
    "\\begin{equation}\n",
    "    \\nabla \\mathbf{W}^l = (\\mathbf{a}^{l-1})^T \\boldsymbol{\\delta}^l\n",
    "\\end{equation}\n",
    "\n",
    "And again for the first layer we will just have the design matrix:\n",
    "\\begin{equation}\n",
    "    \\nabla \\mathbf{W}^{l=1} = (\\mathbf{X})^T \\boldsymbol{\\delta}^{l=1}\n",
    "\\end{equation}"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "30ff46c87fb2ec28"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We do the same updating the bias, we find the gradient from the derivative \n",
    "\\begin{align*}\n",
    "    \\nabla b_{i}^L = \\frac{\\partial C}{\\partial b^L_{i}} &= \\frac{\\partial C}{\\partial a^L_i} \\frac{\\partial a^L_i}{\\partial b^L_{i}}\n",
    "    \\\\ &= \\frac{\\partial C}{\\partial a^L_i} \\frac{\\partial a^L_i}{\\partial z^L_{i}} \\frac{\\partial z^L_{i}}{\\partial b^L_{i}}\n",
    "    \\\\ &= \\delta_i^L \\frac{\\partial z^L_{i}}{\\partial b^L_{i}}\n",
    "\\end{align*}\n",
    "we only need to find the third derivate which, again from $z^L_i = \\sum_j \\left( w^L_{ij}a^{L-1}_j + b^L_i\\right)$, will be\n",
    "\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial z^L_{i}}{\\partial b^L_{i}} = 1\n",
    "\\end{align*}\n",
    "\n",
    "such that the bias gradient is easily\n",
    " \\begin{align*}\n",
    "    \\nabla b_{i}^L = \\delta_i^L\n",
    "\\end{align*}\n",
    "\n",
    "or in matrix notification as\n",
    "\\begin{equation}\n",
    "    \\nabla \\mathbf{b}^L = \\sum_{i=1}^{n_{inputs}} \\boldsymbol{\\delta}^L\n",
    "\\end{equation}\n",
    "\n",
    "and the same for the general layer $l$ we get\n",
    "\\begin{equation}\n",
    "    \\nabla \\mathbf{b}^l = \\sum_{i=1}^{n_{inputs}} \\boldsymbol{\\delta}^l\n",
    "\\end{equation}\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b1895ce78d66b63e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Implementing the analytical expressions needed"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "22b9ceb4c4eceaaa"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "234c7edc39158a7b"
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "outputs": [],
   "source": [
    "def analytic_error_L(target):\n",
    "    \"\"\"Returns function for the analytical expression for output layer error\"\"\"\n",
    "\n",
    "    def func(a):\n",
    "        return a - target\n",
    "\n",
    "    return func\n",
    "\n",
    "\n",
    "def dfdx(a):\n",
    "    \"\"\"Derivative of the activation function;\n",
    "    rewritten expression of the derivative for sigmoid function\"\"\"\n",
    "    return a * (1 - a)\n",
    "\n",
    "\n",
    "analy_err = analytic_error_L(target)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-26T17:36:47.349920900Z",
     "start_time": "2023-10-26T17:36:47.121422Z"
    }
   },
   "id": "9a0522f0ec9e0982"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exploring automatic differentiation for the error calculation\n",
    "We use \n",
    "\\begin{align*}\n",
    "    \\delta_i^L &\\equiv  \\frac{\\partial C}{z_i^L} &= \\frac{\\partial C}{\\partial a^L_i} \\frac{\\partial a^L_i}{\\partial z^L_{i}}\n",
    "    \\\\ &= \\frac{\\partial C}{\\partial a^L_i} \\sigma'(z_i^L)\n",
    "    \\\\ &= \\frac{\\partial C}{\\partial a^L_i} a_i^L (1-a_i^L)\n",
    "\\end{align*}\n",
    "and to potentially save analytical calculation time we try to use automatic differentiation on the cost function to get the derivative to find the error:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "129570a9ac436d4c"
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.50662492]\n",
      " [-0.4252487 ]\n",
      " [-0.46931083]\n",
      " [ 0.60044713]]\n",
      "\n",
      "[[ 0.50662494]\n",
      " [-0.42524868]\n",
      " [-0.46931082]\n",
      " [ 0.6004471 ]]\n"
     ]
    }
   ],
   "source": [
    "def automatic_error_L(target):\n",
    "    \"\"\"Returns function for output layer error using automatic differentiation with Jax.\"\"\"\n",
    "    dCda_L = grad(cost_log_reg(target))\n",
    "\n",
    "    def func(a):\n",
    "        return dCda_L(a) * dfdx(a)\n",
    "\n",
    "    return func\n",
    "\n",
    "\n",
    "# Compare analytical vs automatic error with target predictions\n",
    "auto_err = automatic_error_L(target)\n",
    "print(analy_err(probabilities))\n",
    "print()\n",
    "print(auto_err(probabilities))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-26T17:36:47.379864100Z",
     "start_time": "2023-10-26T17:36:47.124109500Z"
    }
   },
   "id": "6a50c579cef65872"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We see they give the same results, so we could potentially save time if we hadn't already did the calculations. In the rest of the code we will just use the analytical expression since it is faster than using automatic differentiation each run:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "871b67c3caada094"
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "outputs": [],
   "source": [
    "# Choose error method\n",
    "error_func = analy_err  # [analy_err / auto_err]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-26T17:36:47.379864100Z",
     "start_time": "2023-10-26T17:36:47.143457Z"
    }
   },
   "id": "56a2642d2fcb4ca0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Back propagation\n",
    "Since we only have one hidden layer we need to propogate only once from the output layer to the first hidden layer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5dc14ab235c1c487"
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "outputs": [],
   "source": [
    "def back_propagate(X):\n",
    "    \"\"\"Back propagation algorithm for one hidden layer\"\"\"\n",
    "    a_h, probabilities = feed_forward(X)\n",
    "\n",
    "    # Output layer error delta^L\n",
    "    error_o = error_func(probabilities)\n",
    "\n",
    "    # Hidden layer error delta^1\n",
    "    error_h = error_o @ output_weights.T * dfdx(a_h)\n",
    "\n",
    "    # Gradients for the hidden layer\n",
    "    hidden_weights_gradient = X.T @ error_h\n",
    "    hidden_bias_gradient = np.sum(error_h, axis=0)\n",
    "\n",
    "    # Gradients for the output layer\n",
    "    output_weights_gradient = a_h.T @ error_o\n",
    "    output_bias_gradient = np.sum(error_o, axis=0)\n",
    "\n",
    "    return hidden_weights_gradient, hidden_bias_gradient, output_weights_gradient, output_bias_gradient\n",
    "\n",
    "\n",
    "def reset():\n",
    "    \"\"\"Resets hidden layer's and output layer's weights and biases to random values\"\"\"\n",
    "    # Weights and bias in the hidden layer\n",
    "    global hidden_weights, hidden_bias\n",
    "    hidden_weights = rng.standard_normal((n_features, n_hidden_nodes))  # weights normally distributed\n",
    "    hidden_bias = np.zeros(n_hidden_nodes) + 0.01\n",
    "\n",
    "    # Weights and bias in the output layer\n",
    "    global output_weights, output_bias\n",
    "    output_weights = rng.standard_normal((n_hidden_nodes, n_categories))  # weights normally distributed\n",
    "    output_bias = np.zeros(n_categories) + 0.01"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-26T17:36:47.382858400Z",
     "start_time": "2023-10-26T17:36:47.150439400Z"
    }
   },
   "id": "5bb7c334b5bfe1cc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training the neural network until convergence\n",
    "We use the gradients from the back propagation algorithm, then we update the weights and biases with the learning rate $\\eta$. We also add a regularization hyperparameter $\\lambda$ to hopefully improve performance. We do a convergence check on all weights and biases to stop the training."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1e514c3db4789bba"
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training did not converge after max_epochs=1000.\n",
      "\n",
      "After back propagation:\n",
      "Output weights and bias:\n",
      "[[ 6.53910423]\n",
      " [-6.5384835 ]]\n",
      "[-3.31166476]\n",
      "\n",
      "Hidden weights and bias:\n",
      "[[-4.16445223 -4.16778251]\n",
      " [-4.16445223 -4.16778251]]\n",
      "[6.68832782 1.64163974]\n",
      "\n",
      "Target:\n",
      "[[0]\n",
      " [1]\n",
      " [1]\n",
      " [0]]\n",
      "\n",
      "Probabilities:\n",
      "[[0.09463075]\n",
      " [0.9053455 ]\n",
      " [0.9053455 ]\n",
      " [0.09467114]]\n",
      "\n",
      "Prediction:\n",
      "[[0]\n",
      " [1]\n",
      " [1]\n",
      " [0]]\n",
      "\n",
      "Accuracy score of prediction:\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "eta = 0.5  # learning rate\n",
    "lmbd = 0.01  # regularization rate\n",
    "n_epochs = 1000  # max no. epochs for training\n",
    "\n",
    "\n",
    "def train(eta, lmbd, max_epochs=1000, abs_tol=1e-5, print_=True):\n",
    "    \"\"\"Trains the neural network by back propagation until convergence with a given tolerance or until a max number of epoch iterations.\"\"\"\n",
    "    w_h = hidden_weights\n",
    "    b_h = hidden_bias\n",
    "    w_o = output_weights\n",
    "    b_o = output_bias\n",
    "\n",
    "    for i in range(max_epochs):\n",
    "        # Feed forward and back propagate back to get all gradients for each epoch\n",
    "        dW_h, dB_h, dW_o, dB_o = back_propagate(X)\n",
    "\n",
    "        # Regularization term\n",
    "        dW_h += lmbd * w_h\n",
    "        dW_o += lmbd * w_o\n",
    "\n",
    "        # Get total changes for convergence check\n",
    "        change_w_h = eta * dW_h\n",
    "        change_b_h = eta * dB_h\n",
    "        change_w_o = eta * dW_o\n",
    "        change_b_o = eta * dB_o\n",
    "\n",
    "        # Convergence check: check if changes are small enough\n",
    "        if all(\n",
    "                (\n",
    "                        np.all(abs(change_w_h) <= abs_tol),\n",
    "                        np.all(abs(change_b_h) <= abs_tol),\n",
    "                        np.all(abs(change_w_o) <= abs_tol),\n",
    "                        np.all(abs(change_b_o) <= abs_tol)\n",
    "                )\n",
    "        ):\n",
    "            if print_:\n",
    "                print(f\"Training converged after {i} epochs.\\n\")\n",
    "            return\n",
    "\n",
    "        # Update weights and biases\n",
    "        w_h -= change_w_h\n",
    "        b_h -= change_b_h\n",
    "        w_o -= change_w_o\n",
    "        b_o -= change_b_o\n",
    "\n",
    "    if print_:\n",
    "        print(f\"Training did not converge after {max_epochs=}.\\n\")\n",
    "\n",
    "\n",
    "reset()\n",
    "train(eta, lmbd, max_epochs=n_epochs)\n",
    "\n",
    "print(\"After back propagation:\")\n",
    "print(\"Output weights and bias:\")\n",
    "print(output_weights)\n",
    "print(output_bias)\n",
    "\n",
    "print()\n",
    "print(\"Hidden weights and bias:\")\n",
    "print(hidden_weights)\n",
    "print(hidden_bias)\n",
    "\n",
    "print(\"\\nTarget:\")\n",
    "print(target)\n",
    "print(\"\\nProbabilities:\")\n",
    "probabilities = feed_forward(X)[-1]\n",
    "print(probabilities)\n",
    "print(\"\\nPrediction:\")\n",
    "prediction = predict(probabilities)\n",
    "print(prediction)\n",
    "print(\"\\nAccuracy score of prediction:\")\n",
    "print(accuracy_score(target, prediction))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-26T17:36:47.422727300Z",
     "start_time": "2023-10-26T17:36:47.158417700Z"
    }
   },
   "id": "54cbc0df5b4c8b9c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We get the correct prediction for both the OR and the AND gates, however the XOR gate gets predicted uncorrectly."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ce753d18b744bdbe"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Plotting prediction accuracy as functions of our hyperparameters\n",
    "I want to find the best combination of hyperparameters $\\eta$ and $\\lambda$. For this I use a seaborn heatmap plot and we calculate the error from our cost function."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "74abed9a40fc46b5"
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[297], line 15\u001B[0m\n\u001B[0;32m     13\u001B[0m             train(eta, lmbd, max_epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1000\u001B[39m, print_\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m     14\u001B[0m             prediction \u001B[38;5;241m=\u001B[39m predict(feed_forward(X)[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m])\n\u001B[1;32m---> 15\u001B[0m             accuracy[i, j] \u001B[38;5;241m=\u001B[39m \u001B[43maccuracy_score\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprediction\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     17\u001B[0m \u001B[38;5;66;03m# Heatmap plot\u001B[39;00m\n\u001B[0;32m     18\u001B[0m sns\u001B[38;5;241m.\u001B[39mheatmap(\n\u001B[0;32m     19\u001B[0m         accuracy,\n\u001B[0;32m     20\u001B[0m         annot\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     25\u001B[0m         cmap\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRdBu\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m     26\u001B[0m )\n",
      "File \u001B[1;32mE:\\Programmer\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:211\u001B[0m, in \u001B[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    205\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    206\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[0;32m    207\u001B[0m         skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[0;32m    208\u001B[0m             prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[0;32m    209\u001B[0m         )\n\u001B[0;32m    210\u001B[0m     ):\n\u001B[1;32m--> 211\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    212\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m InvalidParameterError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    213\u001B[0m     \u001B[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001B[39;00m\n\u001B[0;32m    214\u001B[0m     \u001B[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001B[39;00m\n\u001B[0;32m    215\u001B[0m     \u001B[38;5;66;03m# the name of the estimator by the name of the function in the error\u001B[39;00m\n\u001B[0;32m    216\u001B[0m     \u001B[38;5;66;03m# message to avoid confusion.\u001B[39;00m\n\u001B[0;32m    217\u001B[0m     msg \u001B[38;5;241m=\u001B[39m re\u001B[38;5;241m.\u001B[39msub(\n\u001B[0;32m    218\u001B[0m         \u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter of \u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mw+ must be\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    219\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__qualname__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must be\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    220\u001B[0m         \u001B[38;5;28mstr\u001B[39m(e),\n\u001B[0;32m    221\u001B[0m     )\n",
      "File \u001B[1;32mE:\\Programmer\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:220\u001B[0m, in \u001B[0;36maccuracy_score\u001B[1;34m(y_true, y_pred, normalize, sample_weight)\u001B[0m\n\u001B[0;32m    154\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Accuracy classification score.\u001B[39;00m\n\u001B[0;32m    155\u001B[0m \n\u001B[0;32m    156\u001B[0m \u001B[38;5;124;03mIn multilabel classification, this function computes subset accuracy:\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    216\u001B[0m \u001B[38;5;124;03m0.5\u001B[39;00m\n\u001B[0;32m    217\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    219\u001B[0m \u001B[38;5;66;03m# Compute accuracy for each possible representation\u001B[39;00m\n\u001B[1;32m--> 220\u001B[0m y_type, y_true, y_pred \u001B[38;5;241m=\u001B[39m \u001B[43m_check_targets\u001B[49m\u001B[43m(\u001B[49m\u001B[43my_true\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_pred\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    221\u001B[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001B[0;32m    222\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m y_type\u001B[38;5;241m.\u001B[39mstartswith(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmultilabel\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
      "File \u001B[1;32mE:\\Programmer\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:85\u001B[0m, in \u001B[0;36m_check_targets\u001B[1;34m(y_true, y_pred)\u001B[0m\n\u001B[0;32m     58\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same classification task.\u001B[39;00m\n\u001B[0;32m     59\u001B[0m \n\u001B[0;32m     60\u001B[0m \u001B[38;5;124;03mThis converts multiclass or binary types to a common shape, and raises a\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     82\u001B[0m \u001B[38;5;124;03my_pred : array or indicator matrix\u001B[39;00m\n\u001B[0;32m     83\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     84\u001B[0m check_consistent_length(y_true, y_pred)\n\u001B[1;32m---> 85\u001B[0m type_true \u001B[38;5;241m=\u001B[39m \u001B[43mtype_of_target\u001B[49m\u001B[43m(\u001B[49m\u001B[43my_true\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minput_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43my_true\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     86\u001B[0m type_pred \u001B[38;5;241m=\u001B[39m type_of_target(y_pred, input_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124my_pred\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     88\u001B[0m y_type \u001B[38;5;241m=\u001B[39m {type_true, type_pred}\n",
      "File \u001B[1;32mE:\\Programmer\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\multiclass.py:387\u001B[0m, in \u001B[0;36mtype_of_target\u001B[1;34m(y, input_name)\u001B[0m\n\u001B[0;32m    385\u001B[0m \u001B[38;5;66;03m# Check multiclass\u001B[39;00m\n\u001B[0;32m    386\u001B[0m first_row \u001B[38;5;241m=\u001B[39m y[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m issparse(y) \u001B[38;5;28;01melse\u001B[39;00m y\u001B[38;5;241m.\u001B[39mgetrow(\u001B[38;5;241m0\u001B[39m)\u001B[38;5;241m.\u001B[39mdata\n\u001B[1;32m--> 387\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[43mxp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munique_values\u001B[49m\u001B[43m(\u001B[49m\u001B[43my\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m2\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m (y\u001B[38;5;241m.\u001B[39mndim \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m2\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(first_row) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m):\n\u001B[0;32m    388\u001B[0m     \u001B[38;5;66;03m# [1, 2, 3] or [[1., 2., 3]] or [[1, 2]]\u001B[39;00m\n\u001B[0;32m    389\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmulticlass\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m suffix\n\u001B[0;32m    390\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[1;32mE:\\Programmer\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_array_api.py:262\u001B[0m, in \u001B[0;36m_NumPyAPIWrapper.unique_values\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m    261\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21munique_values\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[1;32m--> 262\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mnumpy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munique\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m<__array_function__ internals>:200\u001B[0m, in \u001B[0;36munique\u001B[1;34m(*args, **kwargs)\u001B[0m\n",
      "File \u001B[1;32mE:\\Programmer\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\arraysetops.py:274\u001B[0m, in \u001B[0;36munique\u001B[1;34m(ar, return_index, return_inverse, return_counts, axis, equal_nan)\u001B[0m\n\u001B[0;32m    272\u001B[0m ar \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39masanyarray(ar)\n\u001B[0;32m    273\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m axis \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 274\u001B[0m     ret \u001B[38;5;241m=\u001B[39m \u001B[43m_unique1d\u001B[49m\u001B[43m(\u001B[49m\u001B[43mar\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_index\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_inverse\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_counts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[0;32m    275\u001B[0m \u001B[43m                    \u001B[49m\u001B[43mequal_nan\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mequal_nan\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    276\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _unpack_tuple(ret)\n\u001B[0;32m    278\u001B[0m \u001B[38;5;66;03m# axis was specified and not None\u001B[39;00m\n",
      "File \u001B[1;32mE:\\Programmer\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\arraysetops.py:336\u001B[0m, in \u001B[0;36m_unique1d\u001B[1;34m(ar, return_index, return_inverse, return_counts, equal_nan)\u001B[0m\n\u001B[0;32m    334\u001B[0m     aux \u001B[38;5;241m=\u001B[39m ar[perm]\n\u001B[0;32m    335\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 336\u001B[0m     ar\u001B[38;5;241m.\u001B[39msort()\n\u001B[0;32m    337\u001B[0m     aux \u001B[38;5;241m=\u001B[39m ar\n\u001B[0;32m    338\u001B[0m mask \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mempty(aux\u001B[38;5;241m.\u001B[39mshape, dtype\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39mbool_)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "eta_vals = np.array([0.01, 0.05, 0.1, 0.3, 0.5, 0.7, 0.9])  # learning rate \n",
    "lmbd_vals = np.array([1e-5, 1e-4, 1e-3, 0.01, 0.1, 0.3, 0.5, 0.7, 0.9]\n",
    ")  \n",
    "n_epochs = 100  # epochs for training\n",
    "\n",
    "# Iterate through parameters, train, save error to heatmap\n",
    "accuracy = np.zeros((eta_vals.size, lmbd_vals.size))\n",
    "for i, eta in enumerate(eta_vals):\n",
    "    for j, lmbd in enumerate(lmbd_vals):\n",
    "        for k in range(n_epochs):\n",
    "            reset()\n",
    "            train(eta, lmbd, max_epochs=1000, print_=False)\n",
    "            prediction = predict(feed_forward(X)[-1])\n",
    "            accuracy[i, j] = accuracy_score(target, prediction)\n",
    "\n",
    "# Heatmap plot\n",
    "sns.heatmap(\n",
    "        accuracy,\n",
    "        annot=True,\n",
    "        fmt=\".2f\",\n",
    "        xticklabels=[f\"{lmbd:g}\" for lmbd in lmbd_vals],\n",
    "        yticklabels=[f\"{eta:g}\" for eta in eta_vals],\n",
    "        cbar_kws={\"label\": \"Accuracy score\"},\n",
    "        cmap=\"RdBu\",\n",
    ")\n",
    "\n",
    "plt.title(f\"{target_gate}-gate prediction accuracy with max {n_epochs} epochs\")\n",
    "plt.xlabel(\"$\\lambda$\")\n",
    "plt.ylabel(\"$\\eta$\");"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-26T17:43:40.275992100Z",
     "start_time": "2023-10-26T17:42:56.369853700Z"
    }
   },
   "id": "9bbe7dd618326b92"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "4e2b2f706db7f707"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
