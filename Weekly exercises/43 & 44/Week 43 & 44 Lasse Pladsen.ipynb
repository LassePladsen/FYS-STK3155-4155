{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Exercises week 43 and 44\n",
    "### The OR, AND, and XOR gates\n",
    "\n",
    "We have two input values $x_1$ and $x_2$ which decide the output from the two types of gates. Since each input value can be either 0 or 1 we can write the input as a design matrix $X$ where the first and second column represents $x_1$ and $x_2$ respectively as:\n",
    "$$X = \\begin{bmatrix} 0 & 0 \\\\ 0 & 1 \\\\ 1 & 0 \\\\ 1 & 1 \\end{bmatrix}$$\n",
    "\n",
    "The output $y$ for the different gates we can write as the vectors $y^T=[0, 1,1,1]$ for the OR gate, $y^T=[0,0,0,1]$ for the AND gate, and $y^T=[0, 1, 1, 0]$ for the XOR gate. We setup this matrix and these vectors:\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bf257eb40befa320"
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "from jax import grad\n",
    "\n",
    "# Set up design matrix and output vectors\n",
    "X = np.asarray([\n",
    "    [0, 0],\n",
    "    [0, 1],\n",
    "    [1, 0],\n",
    "    [1, 1]\n",
    "])\n",
    "\n",
    "# Gate target arrays\n",
    "yOR = np.asarray([0, 1, 1, 1]).reshape(-1, 1)\n",
    "yAND = np.asarray([0, 0, 0, 1]).reshape(-1, 1)\n",
    "yXOR = np.asarray([0, 1, 1, 0]).reshape(-1, 1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T13:54:22.489240600Z",
     "start_time": "2023-10-25T13:54:22.398742900Z"
    }
   },
   "id": "58160402d5f28b1c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We create our NN architecture with the Sigmoid function $\\sigma$ as activation function where\n",
    "\\begin{equation}\n",
    "    \\sigma(x) = \\frac{1}{1+e^{-x}}\n",
    "\\end{equation}\n",
    "as such"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "40a26ec520420a39"
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before back propagation:\n",
      "\n",
      "Output weights and bias:\n",
      "[[-0.77586755]\n",
      " [ 0.8087058 ]]\n",
      "[0.01]\n",
      "\n",
      "Hidden weights and bias:\n",
      "[[ 0.60172129  1.15161897]\n",
      " [-1.35946236  0.22205533]]\n",
      "[0.01 0.01]\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "n_hidden_nodes = 2  # hidden nodes per layer\n",
    "n_categories = 1  # output nodes\n",
    "n_inputs, n_features = X.shape  # 2 inputs, 4 features\n",
    "target_gate = \"XOR\"  # choose which target gate to train on\n",
    "\n",
    "\n",
    "# Activation function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "# Initialize random number generator with seed\n",
    "rng = np.random.default_rng(2023)\n",
    "\n",
    "# Weights and bias in the hidden layer\n",
    "hidden_weights = rng.standard_normal((n_features, n_hidden_nodes))  # weights normally distributed\n",
    "hidden_bias = np.zeros(n_hidden_nodes) + 0.01\n",
    "\n",
    "# Weights and bias in the output layer\n",
    "output_weights = rng.standard_normal((n_hidden_nodes, n_categories))  # weights normally distributed\n",
    "output_bias = np.zeros(n_categories) + 0.01\n",
    "\n",
    "\n",
    "print(\"Before back propagation:\\n\")\n",
    "print(\"Output weights and bias:\")\n",
    "print(output_weights)\n",
    "print(output_bias)\n",
    "\n",
    "print()\n",
    "print(\"Hidden weights and bias:\")\n",
    "print(hidden_weights)\n",
    "print(hidden_bias)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T13:54:22.568833700Z",
     "start_time": "2023-10-25T13:54:22.414202900Z"
    }
   },
   "id": "cfd0e5879201f65e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Feed forward\n",
    "Then we set up the feed forward algorithm and compare one pass with the target vectors $y^T$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "90be2689e40d3f08"
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "outputs": [],
   "source": [
    "# Get the chosen target array\n",
    "if target_gate == \"OR\":\n",
    "    target = yOR\n",
    "elif target_gate == \"AND\":\n",
    "    target = yAND\n",
    "elif target_gate == \"XOR\":\n",
    "    target = yXOR\n",
    "else:\n",
    "    raise ValueError(f\"Target gate not found ('{target_gate}').\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T13:54:22.610140100Z",
     "start_time": "2023-10-25T13:54:22.441830Z"
    }
   },
   "id": "56fac192b81a286c"
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target:\n",
      "[[0]\n",
      " [1]\n",
      " [1]\n",
      " [0]]\n",
      "\n",
      "Probabilities:\n",
      "[[0.50662492]\n",
      " [0.5747513 ]\n",
      " [0.53068917]\n",
      " [0.60044713]]\n",
      "\n",
      "Prediction:\n",
      "[[1]\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "def feed_forward(X):\n",
    "    \"\"\"Feed forward algorithm for one hidden layer\"\"\"\n",
    "    # Weighted sum of inputs to the hidden layer\n",
    "    z_h = X @ hidden_weights + hidden_bias\n",
    "\n",
    "    # Activation in the hidden layer\n",
    "    a_h = sigmoid(z_h)\n",
    "\n",
    "    # Weighted sum of inputs to the output layer\n",
    "    z_o = a_h @ output_weights + output_bias\n",
    "\n",
    "    # Axis 0 holds each input and axis 1 the probabilities of each category\n",
    "    probabilities = sigmoid(z_o)  # this is a_o\n",
    "    return z_h, a_h, z_o, probabilities\n",
    "\n",
    "\n",
    "@np.vectorize\n",
    "def predict(probability):\n",
    "    \"\"\"Get prediction from array with floats.\"\"\"\n",
    "    if probability < 0.5:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "\n",
    "# Make prediction and compare with gate target y_vectors\n",
    "z_h, a_h, z_o, probabilities = feed_forward(X)\n",
    "predictions = predict(probabilities)\n",
    "\n",
    "print(\"Target:\")\n",
    "print(target)\n",
    "\n",
    "print(\"\\nProbabilities:\")\n",
    "print(probabilities)\n",
    "\n",
    "print(\"\\nPrediction:\")\n",
    "print(predictions)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T13:54:22.626011600Z",
     "start_time": "2023-10-25T13:54:22.474205200Z"
    }
   },
   "id": "197d2b7a7bd3be38"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We see this prediction does not match any target. This is because we only did one pass and that was with random starting weights. Now we setup the cost function and the back propagation algorithm.\n",
    "\n",
    "<!--- \n",
    "For the cost function we use the cross entropy for binary classification given as\n",
    "$$C(\\boldsymbol{\\theta}) = -\\sum_{i=1}^n \\left( y_i \\ln [p(y_i | x_i, \\boldsymbol{\\theta)}] + (1-y_i)\\ln[1-p(y_i | x_i, \\boldsymbol{\\theta)}] \\right)$$\n",
    "\n",
    "where the probabilities $p$ we have from the sigmoid function \n",
    "$$p(y_i=1|x_i, \\boldsymbol{\\theta}) = \\frac{\\exp(\\theta_0 + \\theta_1 x_i)}{1- \\exp(\\theta_0 + \\theta_1 x_i)}$$\n",
    "$$p(y_i=0|x_i, \\boldsymbol{\\theta}) = 1 - p(y_i=1|x_i, \\boldsymbol{\\theta})$$ \n",
    "-->\n",
    "\n",
    "For the cost function we use the cross entropy for binary classification given as\n",
    "$$C(\\boldsymbol{W}) = -\\sum_{i=1}^n \\left( t_i \\log a_i^L + (1-t_i)\\log(1-a_i^L) \\right)$$\n",
    "where $t$ is the target and $a^L$ is the final activation from the final/output layer."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b24a0a554fb220f"
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "outputs": [
    {
     "data": {
      "text/plain": "Array(41.446533, dtype=float32)"
     },
     "execution_count": 466,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cost_log_reg(target):\n",
    "    \"\"\"Returns a function for the logistic cross entropy for binary classification / log loss function using a given target vector.\"\"\"\n",
    "    d = 1e-9  # small value to avoid infinities\n",
    "\n",
    "    def func(x):\n",
    "        #     return -(1 / target.size) * jnp.sum(target * jnp.log(x + d))\n",
    "        #     return -np.sum(target * jnp.log(x + d) + (1 - target) * jnp.log(1 - x + d))\n",
    "        return -jnp.sum(\n",
    "                (target * jnp.log(x + d)) + ((1 - target) * jnp.log(1 - x + d))\n",
    "        )\n",
    "\n",
    "    return func\n",
    "\n",
    "\n",
    "cost_func = cost_log_reg(target)\n",
    "cost_func(predictions)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T13:54:22.629049900Z",
     "start_time": "2023-10-25T13:54:22.496222500Z"
    }
   },
   "id": "e46a022e02ed29b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Calculating the analytical gradients for back propagation\n",
    "We differentiate the cost function with regards to (wgt) the activation of the output layer $a_i^L$ and get:\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial C}{\\partial a_i^L} &= -\\frac{\\partial}{\\partial a_i^L}(t_i \\ln(a_i^L) + (1-t_i)\\ln(1-a_i^L))\n",
    "    \\\\ &= -(\\frac{t_i}{a_i^L} + \\frac{1-t_i}{1-a_i^L}(-1))\n",
    "    \\\\ &= \\frac{1-t_i}{1-a_i^L} - \\frac{t_i}{a_i^L}\n",
    "    \\\\ &= \\frac{a_i^L(1-t_i)}{a_i^L(1-a_i^L)} - \\frac{t_i(1-a_i^L)}{a_i^L(1-a_i^L)}\n",
    "    \\\\ &= \\underline{\\frac{a_i^L-t_i}{a_i^L(1-a_i^L)}}\n",
    "\\end{align*}\n",
    "\n",
    "The expression for the output error $\\delta^L$ is \n",
    "\n",
    "\\begin{equation}\n",
    "    \\delta_i^L = \\sigma'(z_i^L)\\frac{\\partial C}{\\partial a_i^L}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\sigma$ is our Sigmoid function, we can differentiate it to be\n",
    "\n",
    "\\begin{equation}\n",
    "    \\sigma'(x) = \\frac{e^{-x}}{(1+e^{-x})^2}\n",
    "\\end{equation}\n",
    "which gives the following when putting in $a_i^L=\\sigma(a_i^L)=1/(1+e^{-z_i^L})$:\n",
    "\\begin{gather*}\n",
    "    \\sigma'(z_i^L) &= \\frac{e^{-z_i^L}}{(1+e^{-z_i^L})^2}\n",
    "    \\\\ &= \\left( \\frac{1}{a_i^L} - 1 \\right) (a_i^L)^2\n",
    "    \\\\ &= \\underline{a_i^L(1-a_i^L)}\n",
    "\\end{gather*}\n",
    "\n",
    "which gives \n",
    "\\begin{align*}\n",
    "    \\delta_i^L &= a_i^L(1-a_i^L)\\ \\frac{a_i^L-t_i}{a_i^L(1-a_i^L)}\n",
    "    \\\\ &= \\underline{a_i^L-t_i}\n",
    "\\end{align*}\n",
    "\n",
    "such that our gradient for updating the weights will be:\n",
    "\\begin{equation}\n",
    "    \\nabla^L=\\delta_i^L a_i^{L-1} = \\underline{a_i^{L-1}(a_i^L-t_i)}\n",
    "\\end{equation}\n",
    "or substituting $L$ into $o$ for our case with one hidden layer and the output layer $L$ we have \n",
    "\\begin{equation}\n",
    "    \\nabla^o=\\delta_i^o a_i^{h} = \\underline{a_i^{h}(a_i^o-t_i)}\n",
    "\\end{equation}\n",
    "but for our only hidden layer we have\n",
    "\\begin{equation}\n",
    "    \\nabla^h = \\nabla^o (w^o)^T \\sigma'(z^L) = \\underline{\\nabla^o (w^o)^T a^L(1-a^L)}\n",
    "\\end{equation}\n",
    "and the gradients for the biases will be\n",
    "\\begin{align}\n",
    "    \\nabla_k^o &= \\underline{\\sum_i \\delta_{ik}^o}\n",
    "    \\\\ \\nabla_k^h &= \\underline{\\sum_i \\delta_{ik}^h}\n",
    "\\end{align}"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "318e19592a808453"
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "outputs": [],
   "source": [
    "def analytic_error(target):\n",
    "    \"\"\"Analytical output layer error\"\"\"\n",
    "\n",
    "    def func(a):\n",
    "        return a - target\n",
    "\n",
    "    return func\n",
    "\n",
    "\n",
    "analy_grad = analytic_error(target)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T13:54:22.629049900Z",
     "start_time": "2023-10-25T13:54:22.537041Z"
    }
   },
   "id": "9a0522f0ec9e0982"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Using automatic differentiation for the gradient"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "129570a9ac436d4c"
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.50662492]\n",
      " [-0.4252487 ]\n",
      " [-0.46931083]\n",
      " [ 0.60044713]]\n",
      "\n",
      "[[ 0.50662494]\n",
      " [-0.42524868]\n",
      " [-0.46931082]\n",
      " [ 0.6004471 ]]\n"
     ]
    }
   ],
   "source": [
    "def dsigmoid_dx(a):\n",
    "    \"\"\"Rewritten derivative of sigmoid function\"\"\"\n",
    "    return a * (1 - a)\n",
    "\n",
    "\n",
    "def automatic_error(target):\n",
    "    \"\"\"Automatic output layer error\"\"\"\n",
    "    dCda_L = grad(cost_log_reg(target))\n",
    "\n",
    "    def func(a):\n",
    "        return dsigmoid_dx(a) * dCda_L(a)\n",
    "\n",
    "    return func\n",
    "\n",
    "\n",
    "auto_grad = automatic_error(target)\n",
    "\n",
    "# Compare analytical vs automatic with target predictions\n",
    "print(analy_grad(probabilities))\n",
    "print()\n",
    "print(auto_grad(probabilities))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T13:54:22.779143400Z",
     "start_time": "2023-10-25T13:54:22.557865400Z"
    }
   },
   "id": "6a50c579cef65872"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Back propagation\n",
    "The results are the same, so I use the analytical expression since it is faster than using autograd each run.\n",
    "\n",
    "Since we only have one hidden layer we need to propogate only once from the output layer to the first hidden layer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5dc14ab235c1c487"
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "outputs": [],
   "source": [
    "# Choose gradient method\n",
    "error_func = analy_grad\n",
    "\n",
    "\n",
    "def back_propagate(X):\n",
    "    \"\"\"Back propagation algorithm for one hidden layer\"\"\"\n",
    "    z_h, a_h, z_o, probabilities = feed_forward(X)\n",
    "\n",
    "    # Output layer error delta^L\n",
    "    error_o = error_func(probabilities)\n",
    "\n",
    "    # Hidden layer error delta^1\n",
    "    error_h = error_o @ output_weights.T * dsigmoid_dx(a_h)\n",
    "\n",
    "    # Gradients for the hidden layer\n",
    "    hidden_weights_gradient = X.T @ error_h\n",
    "    hidden_bias_gradient = np.sum(error_h, axis=0)\n",
    "\n",
    "    # Gradients for the output layer\n",
    "    output_weights_gradient = a_h.T @ error_o\n",
    "    output_bias_gradient = np.sum(error_o, axis=0)\n",
    "\n",
    "    return hidden_weights_gradient, hidden_bias_gradient, output_weights_gradient, output_bias_gradient"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T13:54:22.780137900Z",
     "start_time": "2023-10-25T13:54:22.645965400Z"
    }
   },
   "id": "5bb7c334b5bfe1cc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Iterating until convergence"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1e514c3db4789bba"
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After back propagation:\n",
      "\n",
      "Output weights and bias:\n",
      "[[-7.03235395]\n",
      " [ 7.5889668 ]]\n",
      "[-0.55613141]\n",
      "\n",
      "Hidden weights and bias:\n",
      "[[ 8.5189733   8.54247513]\n",
      " [-4.14780657  4.89743741]]\n",
      "[ 2.64258752 -1.4108913 ]\n",
      "\n",
      "Target\n",
      "[[0]\n",
      " [1]\n",
      " [1]\n",
      " [0]]\n",
      "\n",
      "Probabilities:\n",
      "[[0.00356487]\n",
      " [0.99604986]\n",
      " [0.4986298 ]\n",
      " [0.50168888]]\n",
      "\n",
      "Prediction\n",
      "[[0]\n",
      " [1]\n",
      " [0]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "eta = 0.5  # learning rate\n",
    "tol = 1e-7  # convergence check tolerance \n",
    "n_epochs = 1000  # max number of epochs \n",
    "\n",
    "for i in range(n_epochs):\n",
    "    # Feed forward and back propagate back to get all gradients for each epoch\n",
    "    dWh, dBh, dWo, dBo = back_propagate(X)\n",
    "    \n",
    "    # Update weights and biases\n",
    "    output_weights -= eta * dWo\n",
    "    output_bias -= eta * dBo\n",
    "    hidden_weights -= eta * dWh\n",
    "    hidden_bias -= eta * dBh\n",
    "\n",
    "print(\"After back propagation:\\n\")\n",
    "print(\"Output weights and bias:\")\n",
    "print(output_weights)\n",
    "print(output_bias)\n",
    "\n",
    "print()\n",
    "print(\"Hidden weights and bias:\")\n",
    "print(hidden_weights)\n",
    "print(hidden_bias)\n",
    "\n",
    "print(\"\\nTarget\")\n",
    "print(target)\n",
    "print(\"\\nProbabilities:\")\n",
    "probabilities = feed_forward(X)[-1]\n",
    "print(probabilities)\n",
    "print(\"\\nPrediction\")\n",
    "print(predict(probabilities))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T13:54:23.075563600Z",
     "start_time": "2023-10-25T13:54:22.914254500Z"
    }
   },
   "id": "54cbc0df5b4c8b9c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We get the correct result for both the OR and the AND gates, however the XOR gate we do not get the correct prediction. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ce753d18b744bdbe"
  },
  {
   "cell_type": "markdown",
   "source": [
    "[PLOT ACCURACY AS FUNCTION OF LEARNING RATE]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "74abed9a40fc46b5"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "4e2b2f706db7f707"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
